[{"content":"Where the content is too good to not watch them all\nhttps://www.youtube.com/@Computerphile/videos\nhttps://www.youtube.com/@3blue1brown\nhttps://www.youtube.com/@Fireship\n","permalink":"http://localhost:1313/posts/youtube-rabbit-holes/","summary":"Where the content is too good to not watch them all\nhttps://www.youtube.com/@Computerphile/videos\nhttps://www.youtube.com/@3blue1brown\nhttps://www.youtube.com/@Fireship","title":"Youtube Rabbit Holes"},{"content":"Motivation I recently completed my first semester at Georgia Tech in which I took the AI course offered by Thad Starner. Instead of moving right into another course for the summer semester I decided to use this summer to apply what I learned to a personal project.\nThe most interesting topic I thought we covered was the use of Hidden Markov Models (HMMs) and how they can be used for speech recognition.\nBefore I even got started Hidden Markov Models wikipedia\nThe idea of a probability distribution\nThe idea of a probability density function\nRandom variable definition\nUnderstanding the architecture Hidden Markov Models are a part of ASR systems but what is their role? It turns out that the machine learning archiecture for an ASR is much more than just an HMM.\nAfter scouting for some resources on ASR system\u0026rsquo;s I came across Preethi Jyothi\u0026rsquo;s Automatic Speech Recongition overview. This helped me understand the various pieces that come together to produce inference on a speech wave. I went through and did a thourgh mapping of the archtecture but unfortunately towards the end of the overview it was expressed that this architecture is outdated. All I can say is it helped me build a better intuition for understanding machine learning models.\nHow to develop a personal project As an engineer the correct approach to building a car would be to start with roller blades, then make a scooter, then a bike, then a car.\nI\u0026rsquo;ve fallen into the temptation (like I assume many of you reading have as well) to want the car right away. After many half-built projects I am doing my best to listen to my cost function and strive for minimum viable products. That being said I wanted to build an ASR system from scratch but in reality it would be better to use this as a learning process and create something simple or use some out of the box options.\nAfter researching for open source speech recongition toolkits I decided to use Kaldi.\nKaldi A \u0026ldquo;recipe\u0026rdquo; refers to a set of scripts and configurations used to process and train speech recongition models. These recipes outline the steps and parameters requried for tasks such as data preperation, feature extraction, model traning and evaluation.\nI originally went through the Kaldi tutorial by the end of which I realized that I would need access to the Linguistic Data Consortium, which costs more than $1000 to have a membership to if your institution does not provide you access. This was not a total time sink as there was a subsequent and appropriately named Kaldi for dummies tutorial which is a much more straight forward approach to getting Kaldi setup but with the caveate that you must provide your own data. So that was my first task, find some data.\nreferences https://www.youtube.com/watch?v=q67z7PTGRi8\u0026amp;t https://en.wikipedia.org/wiki/Hidden_Markov_model https://mathinsight.org/probability_distribution_idea ","permalink":"http://localhost:1313/posts/asr/","summary":"Motivation I recently completed my first semester at Georgia Tech in which I took the AI course offered by Thad Starner. Instead of moving right into another course for the summer semester I decided to use this summer to apply what I learned to a personal project.\nThe most interesting topic I thought we covered was the use of Hidden Markov Models (HMMs) and how they can be used for speech recognition.","title":"Automatic Speech Recognition (ASR) systems"},{"content":"about page ","permalink":"http://localhost:1313/about/","summary":"about","title":"About"}]